{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c295abce",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 23/24\n",
    "\n",
    "# Home Assignment 2 (26 pts)\n",
    "\n",
    "Submit your solution via Ilias until 23.59h on Wednesday, October 25th. Late submissions are accepted until 12:00am on the following day, with 1/4 of the total possible points deducted from the score.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
    "\n",
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions. \n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b9dc97056fbff",
   "metadata": {},
   "source": [
    "#### Team members and studentIDs:\n",
    "* Kok Teng Ng, 1936360\n",
    "* I-Chen Hsieh, 1870180\n",
    "* Hyewon Kang, 1980634\n",
    "* Minjeong Lee, 1978925"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a19c09",
   "metadata": {},
   "source": [
    "Additional packages (if any):\n",
    " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5e2d05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:09.840969Z",
     "start_time": "2023-10-20T19:46:09.722490Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edf14c",
   "metadata": {},
   "source": [
    "### Task 1: WordNet word similarity (9 points)\n",
    "\n",
    "In this task, we want to implement the similarity between two words in WordNet (https://www.nltk.org/api/nltk.corpus.reader.wordnet.html) using the NLTK package. The word similarity between two words is given by\n",
    "$$\n",
    "\\frac{1}{1+d}\n",
    "$$\n",
    "where $d$ is the distance of the shortest path in the hypernym/hyponym hierarchy tree in WordNet between any pair of synsets that are associated with the two input words.\n",
    "\n",
    "From NLTK you should __only__ use the `synsets`, `hypernyms` and `instance_hpyernyms` functions.\n",
    "\n",
    "The following subtasks build on each other, i.e. the functions of the preceding subtasks can be used for the current subtask.\n",
    "\n",
    "_Note: the distance of a synset to itself is 0, the distance to a direct hypernym is 1, ..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2b0d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:10.437397Z",
     "start_time": "2023-10-20T19:46:09.727210Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6c82f",
   "metadata": {},
   "source": [
    "__a)__ Write a function ``shortest_paths_to`` that takes a synset as input and computes the shortest paths to all nodes on the way to the root in the hypernym hierarchy tree of WordNet. The function should return a dictionary that matches all visited hypernyms on the way(s) to the root to the distance of the shortest path from the input synset. Consider that a synset might have multiple paths to the root and that some nodes might appear in multiple paths. However, we only want to store the shortest distances. Moreover, keep in mind that the input synset might be an instance. __(3 pts)__\n",
    "\n",
    "Use the signature in the cell below.\n",
    "\n",
    "__Example:__ _Calling_ ``shortest_paths_to(s)`` _on the synset_ ``s = wn.synset('calculator.n.01')`` _should yield the following result:_\n",
    "\n",
    "``\n",
    "{Synset('calculator.n.01'): 0,\n",
    " Synset('expert.n.01'): 1,\n",
    " Synset('person.n.01'): 2,\n",
    " Synset('causal_agent.n.01'): 3,\n",
    " Synset('organism.n.01'): 3,\n",
    " Synset('physical_entity.n.01'): 4,\n",
    " Synset('living_thing.n.01'): 4,\n",
    " Synset('entity.n.01'): 5,\n",
    " Synset('whole.n.02'): 5,\n",
    " Synset('object.n.01'): 6}\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d66e04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:10.451157Z",
     "start_time": "2023-10-20T19:46:10.442553Z"
    }
   },
   "outputs": [],
   "source": [
    "def shortest_paths_to(start_syn: Synset) -> Dict[Synset, int]:\n",
    "    \"\"\"Compute the shortest distance to all nodes on paths to the root.\n",
    "    :param start_syn: synset to which we want to compute the shortest distances\n",
    "    :return: dict that matches all visited hypernyms to their distance to the input synset  \n",
    "    \"\"\" \n",
    "    # your code here\n",
    "    \n",
    "    # initialize visit_node and distances\n",
    "    visit_node = [(start_syn, 0)]\n",
    "    distances = {start_syn: 0}\n",
    "    \n",
    "    while visit_node: # loop until visit_node is empty\n",
    "        # delete visited node for visit the next node\n",
    "        cur_synset, cur_distance = visit_node.pop(0)\n",
    "\n",
    "        # get all hypernyms and instance hypernyms\n",
    "        all_hypernyms = cur_synset.hypernyms() + cur_synset.instance_hypernyms()\n",
    "        \n",
    "        for hypernym in all_hypernyms:\n",
    "            # check if the path is shorter than previously found paths\n",
    "            if hypernym not in distances or cur_distance + 1 > distances[hypernym]:\n",
    "                distances[hypernym] = cur_distance + 1\n",
    "                visit_node.append((hypernym, cur_distance + 1))\n",
    "    \n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2363ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.482605Z",
     "start_time": "2023-10-20T19:46:10.446371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('calculator.n.01'): 0, Synset('expert.n.01'): 1, Synset('person.n.01'): 2, Synset('causal_agent.n.01'): 3, Synset('organism.n.01'): 3, Synset('physical_entity.n.01'): 7, Synset('living_thing.n.01'): 4, Synset('entity.n.01'): 8, Synset('whole.n.02'): 5, Synset('object.n.01'): 6}\n"
     ]
    }
   ],
   "source": [
    "# test for a)\n",
    "test_s = wn.synset('calculator.n.01')\n",
    "result = shortest_paths_to(test_s)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda6f14",
   "metadata": {},
   "source": [
    "__b)__ Write a function ``merge_paths`` that gets two dictionaries that map synsets to shortest distances (you can assume they were created by the function from __a)__) and merges them. The function shold return a dictionary that includes all synsets and distances that appear in any of the input dictionaries. If a synset appears in both input dictionaries, we want to keep the shorter distance. Leave the input dictionaries unaltered. __(1.5 pts)__\n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ffe524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.482856Z",
     "start_time": "2023-10-20T19:46:11.479870Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_paths(p1: Dict[Synset, int], p2: Dict[Synset, int]) -> Dict[Synset, int]:\n",
    "    \"\"\"Merge two paths keeping the shorter distance for synsets that appear more than once.\n",
    "    :param p1: first dict that maps synsets to their shortest distances\n",
    "    :param p2: second dict that maps synsets to their shortest distances\n",
    "    :return: merged dict\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    # initialize merged_paths to p1\n",
    "    merged_paths = p1.copy()\n",
    "    \n",
    "    for synset, distance in p2.items():\n",
    "        # if the synset is not in the merged_paths or the distance in p2 is shorter than current paths, update with the shorter one\n",
    "        if synset not in merged_paths or distance < merged_paths[synset]:\n",
    "            merged_paths[synset] = distance\n",
    "            \n",
    "    return merged_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db7f180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.487275Z",
     "start_time": "2023-10-20T19:46:11.484415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('calculator.n.01'): 0, Synset('expert.n.01'): 1, Synset('person.n.01'): 2}\n"
     ]
    }
   ],
   "source": [
    "# test for b)\n",
    "path1 = {wn.synset('calculator.n.01'): 0, wn.synset('expert.n.01'):1}\n",
    "path2 = {wn.synset('calculator.n.01'): 1, wn.synset('person.n.01'):2}\n",
    "result = merge_paths(path1, path2)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7ce46",
   "metadata": {},
   "source": [
    "__c)__ Write a function ``all_hypernym_paths`` that gets a word as input and returns a dictionary that maps all hypernyms that are reachable from the set of synsets associated with the word to the shortest distance leading there. __(1.5 pts)__\n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13112dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.499426Z",
     "start_time": "2023-10-20T19:46:11.488236Z"
    }
   },
   "outputs": [],
   "source": [
    "def all_hypernym_paths(word: str) -> Dict[Synset, int]:\n",
    "    \"\"\"Get all hypernyms of all synsets associated with the input word and compute the shortest distance leading there.\n",
    "    :param word: input word\n",
    "    :return: dict that matches all reachable hypernyms to their shortest distance \n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    # get synsets and initialize an empty dictionary \n",
    "    synsets = wn.synsets(word)\n",
    "    paths = {}\n",
    "    \n",
    "    for synset in synsets:\n",
    "        # compute the shortest path\n",
    "        cur_path = shortest_paths_to(synset)\n",
    "        # merge the paths\n",
    "        paths = merge_paths(paths, cur_path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c232692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.500331Z",
     "start_time": "2023-10-20T19:46:11.491756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('calculator.n.01'): 0, Synset('expert.n.01'): 1, Synset('person.n.01'): 2, Synset('causal_agent.n.01'): 3, Synset('organism.n.01'): 3, Synset('physical_entity.n.01'): 7, Synset('living_thing.n.01'): 4, Synset('entity.n.01'): 8, Synset('whole.n.02'): 5, Synset('object.n.01'): 6, Synset('calculator.n.02'): 0, Synset('machine.n.01'): 1, Synset('device.n.01'): 2, Synset('instrumentality.n.03'): 3, Synset('artifact.n.01'): 4}\n"
     ]
    }
   ],
   "source": [
    "#test for c)\n",
    "result = all_hypernym_paths('calculator')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49270f8b",
   "metadata": {},
   "source": [
    "__d)__  Write a function ``get_dist`` that returns the word similarity between two input words, according to the formula given in the task description at the beginning.  __(3 pts)__\n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a4e8dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.648473Z",
     "start_time": "2023-10-20T19:46:11.497933Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dist(w1 : str, w2 : str) -> float:\n",
    "    \"\"\"Compute the similarity between two input words in the WordNet hierarchy tree.\n",
    "    :param w1: first input word\n",
    "    :param w2: second input word\n",
    "    :return: word similarity\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    path_w1 = all_hypernym_paths(w1)\n",
    "    path_w2 = all_hypernym_paths(w2)\n",
    "    \n",
    "    hypernyms = set(path_w1.keys()) & set(path_w2.keys())\n",
    "    \n",
    "    # if no common hypernyms, similarity = 0\n",
    "    if not hypernyms:\n",
    "        return 0.0\n",
    "    \n",
    "    shortest_distance = float('inf')\n",
    "    \n",
    "    for hypernym in hypernyms:\n",
    "        distance = path_w1[hypernym] + path_w2[hypernym]\n",
    "        shortest_distance = min(shortest_distance, distance)\n",
    "    \n",
    "    # calculate similarity\n",
    "    similarity = 1 / (1 + shortest_distance)\n",
    "    \n",
    "    return similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b309b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.667305Z",
     "start_time": "2023-10-20T19:46:11.506619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# test for d)\n",
    "w1, w2 = \"calculator\", \"expert\"\n",
    "result = get_dist(w1, w2)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426121d",
   "metadata": {},
   "source": [
    "### Task 2: Lesk algorithm (4 points)\n",
    "\n",
    "In this task we want to implement a simple version of the Lesk algorithm, a thesaurus-based method for word sense disambiguation. Given a target word $w$ and a context, the algorithm finds the word sense that is most fitting in the context. To achieve this, the Lesk algorithm computes the number of overlapping words between the context sentence and the definitions of the WordNet synsets, associated with $w$.\n",
    "\n",
    "Write a function ``lesk`` that takes a word and a context string (e.g. a sentence) and returns the most fitting sense from the synsets associated with the word and the corresponding context overlap. The most fitting sense is the one whose definition shares the most words with the context string. Before matching tokens, make sure to \n",
    "\n",
    "* only include valid tokens (cf. HA 1, task 2a)\n",
    "* remove stopwords\n",
    "* only match stems of words (e.g. consider the ``PorterStemmer`` from ``nltk``)\n",
    "\n",
    "When computing the context overlap, count each stemmed word only once, even if they occur multiple times. If there is no fitting synset, i.e. the context overlap between the context and the synset definitions is 0, return None instead.\n",
    "\n",
    "Use the signature in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "055d9147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.667583Z",
     "start_time": "2023-10-20T19:46:11.507167Z"
    }
   },
   "outputs": [],
   "source": [
    "# HA 1, task 2a)\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "def get_valid_tokens(tokens: Union[List[str], StreamBackedCorpusView], remove_stopwords: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens that should be cleaned\n",
    "    :param remove_stopwords: bool indicating if stopwords should be removed \n",
    "                             False by default\n",
    "    :return: list of valid tokens\n",
    "    \"\"\"\n",
    "    valid = []\n",
    "    punct = string.punctuation\n",
    "    stop = stopwords.words('english')\n",
    "    digit = re.compile(r\"\\d+\")\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in punct:\n",
    "            continue\n",
    "        if remove_stopwords and t.lower() in stop:\n",
    "            continue\n",
    "        if re.fullmatch(digit, t):\n",
    "            continue\n",
    "        valid.append(t.lower())\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "054adffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.669399Z",
     "start_time": "2023-10-20T19:46:11.532771Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lesk(word: str, context: str) -> (Synset, int):\n",
    "    '''\n",
    "    Compute the most probable sense of a word in the given context.\n",
    "    :param word: ambiguous word\n",
    "    :param context: context in which the word appears\n",
    "    :returns: \n",
    "        - synset with the most likely word sense\n",
    "        - context overlap of synset and context          \n",
    "    '''\n",
    "    def preprocess_token(string:str) -> List[str]:\n",
    "        '''\n",
    "        Transform string into valid token list with removing stopwords, stemming, and deduplication.\n",
    "        :param string: word or context\n",
    "        :returns: list of valid tokens\n",
    "        '''\n",
    "        tokens = word_tokenize(string)\n",
    "        valid_tokens = get_valid_tokens(tokens, remove_stopwords = True)\n",
    "        stem_tokens =[]\n",
    "        for t in valid_tokens:\n",
    "            stem_tokens.append(ps.stem(t))\n",
    "        return(set(stem_tokens))\n",
    "\n",
    "    max_overlap = 0\n",
    "    preprocess_context = preprocess_token(context)\n",
    "    for ss in wn.synsets(word):\n",
    "        preprocess_word = preprocess_token(ss.definition())\n",
    "        overlap_count = len(preprocess_word.intersection(preprocess_context))\n",
    "        if overlap_count > max_overlap:\n",
    "            # ignore same overlapping, since only pick one synset\n",
    "            most_fitting_synset = ss.name() \n",
    "            max_overlap = overlap_count\n",
    "    \n",
    "    if max_overlap == 0:\n",
    "        return(None, 0)\n",
    "    else:\n",
    "        return(most_fitting_synset, max_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e03aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.671114Z",
     "start_time": "2023-10-20T19:46:11.535304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tea.n.02', 3)\n",
      "('tea.n.01', 2)\n",
      "(None, 0)\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "print(lesk('tea', 'I love to have tea and cake as meal'))\n",
    "print(lesk('tea', 'Tea leaves'))\n",
    "print(lesk('tea', 'I like apple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23506c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.671755Z",
     "start_time": "2023-10-20T19:46:11.588024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a beverage made by steeping tea leaves in water\n",
      "['a', 'beverage', 'made', 'by', 'steeping', 'tea', 'leaves', 'in', 'water']\n",
      "['beverage', 'made', 'steeping', 'tea', 'leaves', 'water']\n",
      "['beverag', 'made', 'steep', 'tea', 'leav', 'water']\n",
      "{'steep', 'beverag', 'tea', 'made', 'leav', 'water'}\n",
      "1\n",
      "\n",
      "a light midafternoon meal of tea and sandwiches or cakes\n",
      "['a', 'light', 'midafternoon', 'meal', 'of', 'tea', 'and', 'sandwiches', 'or', 'cakes']\n",
      "['light', 'midafternoon', 'meal', 'tea', 'sandwiches', 'cakes']\n",
      "['light', 'midafternoon', 'meal', 'tea', 'sandwich', 'cake']\n",
      "{'midafternoon', 'light', 'meal', 'cake', 'tea', 'sandwich'}\n",
      "3\n",
      "\n",
      "a tropical evergreen shrub or small tree extensively cultivated in e.g. China and Japan and India; source of tea leaves\n",
      "['a', 'tropical', 'evergreen', 'shrub', 'or', 'small', 'tree', 'extensively', 'cultivated', 'in', 'e.g', '.', 'China', 'and', 'Japan', 'and', 'India', ';', 'source', 'of', 'tea', 'leaves']\n",
      "['tropical', 'evergreen', 'shrub', 'small', 'tree', 'extensively', 'cultivated', 'e.g', 'china', 'japan', 'india', 'source', 'tea', 'leaves']\n",
      "['tropic', 'evergreen', 'shrub', 'small', 'tree', 'extens', 'cultiv', 'e.g', 'china', 'japan', 'india', 'sourc', 'tea', 'leav']\n",
      "{'shrub', 'japan', 'tree', 'evergreen', 'cultiv', 'india', 'small', 'e.g', 'tropic', 'extens', 'tea', 'leav', 'sourc', 'china'}\n",
      "1\n",
      "\n",
      "a reception or party at which tea is served\n",
      "['a', 'reception', 'or', 'party', 'at', 'which', 'tea', 'is', 'served']\n",
      "['reception', 'party', 'tea', 'served']\n",
      "['recept', 'parti', 'tea', 'serv']\n",
      "{'serv', 'tea', 'parti', 'recept'}\n",
      "1\n",
      "\n",
      "dried leaves of the tea shrub; used to make tea\n",
      "['dried', 'leaves', 'of', 'the', 'tea', 'shrub', ';', 'used', 'to', 'make', 'tea']\n",
      "['dried', 'leaves', 'tea', 'shrub', 'used', 'make', 'tea']\n",
      "['dri', 'leav', 'tea', 'shrub', 'use', 'make', 'tea']\n",
      "{'shrub', 'make', 'dri', 'tea', 'leav', 'use'}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## checking\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "test = ['tea', 'cake', 'meal']\n",
    "\n",
    "for ss in wn.synsets('tea'):\n",
    "    print(ss.definition())\n",
    "    ss_tokens = word_tokenize(ss.definition())\n",
    "    print(ss_tokens)\n",
    "    ss_valid_tokens = get_valid_tokens(ss_tokens, remove_stopwords = True)\n",
    "    print(ss_valid_tokens)\n",
    "    stem_tokens =[]\n",
    "    for w in ss_valid_tokens:\n",
    "        stem_tokens.append(ps.stem(w))\n",
    "    print(stem_tokens)\n",
    "    print(set(stem_tokens))\n",
    "    print(len(set(stem_tokens).intersection(set(test))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd2570",
   "metadata": {},
   "source": [
    "### Task 3: Markov chains (13 points)\n",
    "\n",
    "In this task we want to create a language model by using the independence assumption af Markov. We therefore assume that the probability of a word is only dependent on a fixed number of preceding words. E.g. by restricting the number of preceding words to $n$ we can approximate the probability of a word $w_{i}$ following a sequence of words $w_1, ..., w_{i-1}$ by:\n",
    "\n",
    "$P(w_{i}|w_1, ..., w_{i-1}) \\approx P(w_{i}|w_{i-n}, ..., w_{i-1})$\n",
    "\n",
    "We will first train our model on a given corpus and then use it to automatically generate text.\n",
    "\n",
    "Throughout this task we will define a single class with different functions. If you're unsure how to access class methods and attributes, take a look at the documentation (https://docs.python.org/3/tutorial/classes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19633b2",
   "metadata": {},
   "source": [
    "__a) Collecting the counts (3 pts)__\n",
    "\n",
    "Write a function `process_corpus` that takes a corpus of text (as a sequence of tokens) as input and counts how often an n-gram of length $n$ (``context_len=n``) is followed by a certain word (the n-grams should __not__ be padded). The function should return a dictionary that maps every n-gram that is observed in the corpus to an inner dictionary. The inner dictionary maps each word to a number, that indicates how often the word succeeds the n-gram in the given corpus. We will need these counts to compute the conditional probabilities $P(w_{i}|w_{i-n}, ..., w_{i-1})$.\n",
    "Additionally, also return the entire vocabulary $V$ (i.e. the set of all unique tokens that appear in the corpus).\n",
    "\n",
    "Make sure your implementation is efficient by using e.g. ``Counter`` and ``defaultdict`` from the package ``collections``.   \n",
    "\n",
    "__b) Conditional probabilities (3 pts)__\n",
    "\n",
    "Write a function `transition_prob` that takes an n-gram $(w_{i-n}, ..., w_{i-1})$ and a word $w_{i}$ of the vocabulary $V$ as input and returns the conditional probability that the given n-gram is followed by the input word $w_{i}$. Recall that this conditional probability can be computed as follows:\n",
    "\n",
    "$P(w_{i}|w_{i-n}, ..., w_{i-1}) = \\frac{\\text{Count}(w_{i-n}, ..., w_{i-1}, w_{i})}{\\sum_{w \\in V}\\text{Count}(w_{i-n}, ..., w_{i-1}, w)}$\n",
    "\n",
    "If the n-gram has never been observed, return $\\frac{1}{|V|}$.\n",
    "\n",
    "__c) Most likely word (3 pts)__\n",
    "\n",
    "Write a function `most_likely_word` that gets an n-gram as input and returns the word that is most likely to succeed the given n-gram. In case there are multiple words that are equally likely to follow the given n-gram, return all of them. \n",
    "Note that you should **not** loop over the **entire** vocabulary to obtain the most likely word.\n",
    "In case the given n-gram has never been observed, return the entire vocabulary.\n",
    "\n",
    "__d) Generating text (2 pts)__\n",
    "\n",
    "Write a function `generate_text` that generates a text sequence of length `k`, given a starting sequence of words (`ngram`). The function should choose the most likely next word in every step; in case there are multiple equally likely words, randomly choose one. You should return a list of ``k`` words, that includes the starting sequence and is the most probable continuation. \n",
    "\n",
    "\n",
    "Please do not implement other functions for the MarkovModel class.\n",
    "\n",
    "Use the function signatures in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304206ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.671940Z",
     "start_time": "2023-10-20T19:46:11.603593Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "\n",
    "# combined with hyewon code\n",
    "class MarkovModel():\n",
    "    '''Markov model for generating text.'''\n",
    "    \n",
    "    def __init__(self, tokens: Sequence[str], context_len: int):\n",
    "        '''\n",
    "        :param tokens: text corpus on which the model is trained on as an iterator of tokens\n",
    "        :param context_len: length of the n-gram (number of preceding words)\n",
    "        '''\n",
    "        self.context_len = context_len \n",
    "        self.counts, self.vocabulary = self.process_corpus(tokens)\n",
    "        \n",
    "    def process_corpus(self, tokens: Sequence[str]) -> (Dict[Tuple[str, ...], Dict[str, int]], Set):\n",
    "        '''Training method of the model, counts the occurences of each word after each observed n-gram.\n",
    "        :param tokens: text corpus on which the model is trained on as an iterator of tokens\n",
    "        :returns: \n",
    "            - nested dict that maps each n-gram to the counts of the words succeeding it\n",
    "            - the whole vocabulary as a set\n",
    "        '''\n",
    "        # define the ngram occurrence dictionary\n",
    "        counts = defaultdict(Counter)\n",
    "        # to return the vocabulary from those given tokens as unique set\n",
    "        vocabulary = set(tokens)\n",
    "        # getting the ngram from the tokens with the following token, don't adding padding to the given tokens\n",
    "        n_grams = list(ngrams(tokens, self.context_len + 1, pad_left = False, pad_right = False))\n",
    "        for ngram in n_grams:\n",
    "            # getting the following token\n",
    "            next_token = ngram[-1]\n",
    "            # getting the given conditional of ngram\n",
    "            context = tuple(ngram[:-1])\n",
    "            # updating the occurrence counter within the dictionary\n",
    "            counts[context][next_token] = counts[context][next_token] + 1\n",
    "        # return occurrence and vocabulary\n",
    "        return counts, vocabulary\n",
    "        \n",
    "    def transition_prob(self, ngram: Tuple[str, ...], word: str) -> float:\n",
    "        '''Compute the conditional probability that the input word follows the given n-gram.\n",
    "        :param ngram: string tuple that represents an n-gram\n",
    "        :param word: input word\n",
    "        :return: probability that the n-gram is followed by the input word\n",
    "        '''\n",
    "        # deal with the assumption for zero frequency of the ngram (returning result as 1/|V|)\n",
    "        if ngram not in self.counts:\n",
    "            return 1/(len(self.vocabulary))\n",
    "        # compute the conditional probability\n",
    "        else:\n",
    "            # if the word are occurring belong to the ngrams, then only compute the conditional probability\n",
    "            if word in self.counts[ngram]:\n",
    "                # compute the joint counting\n",
    "                joint_counting = self.counts[ngram][word]\n",
    "                # to get the evidence occurrence frequency\n",
    "                prior_evidence = sum(self.counts[ngram].values())\n",
    "                # compute the conditional probability\n",
    "                probability = joint_counting / prior_evidence\n",
    "                return probability\n",
    "            # if the word doesn't occur belong to the ngrams, then return 0.00 as probability result\n",
    "            else:\n",
    "                return 0.0\n",
    "            \n",
    "    def most_likely_word(self, ngram: Tuple[str, ...]) -> Set[str]:\n",
    "        '''Computes which word is most likely to follow a given n-gram.\n",
    "        :param ngram: n-gram we are interested in\n",
    "        return: set of words that are most likely to follow the n-gram\n",
    "        '''\n",
    "        # deal with the assumption for zero frequency of the ngram (returning the entire vocabulary)\n",
    "        if ngram not in self.counts:\n",
    "            return set(self.vocabulary)\n",
    "        # else find the more likely bi-gram \n",
    "        else:\n",
    "            # define the probability of ngram and token\n",
    "            probability_of_words = 0.0\n",
    "            # initial the list to store those most likely words that are more likely following the n-gram\n",
    "            likely_words = []\n",
    "            # loop entire vocabulary to compute the probability that the word is likely to follow the given n-gram, then do comparison as select the highest probability word that are following to the according given n-gram\n",
    "            for word in self.vocabulary:\n",
    "                # compute the probability that the word are belong after the n-gram\n",
    "                temp_prob = self.transition_prob(ngram, word)\n",
    "                # compare the probability if current word are more likely to belong to the previous word, then replace it\n",
    "                if temp_prob > probability_of_words:\n",
    "                    # update the probability as the most likely word\n",
    "                    probability_of_words = temp_prob\n",
    "                    # clear the entire list because there might have potential as multiple words have equally likely to follow the n-grams\n",
    "                    likely_words.clear()\n",
    "                    # add the highest probability word into the list\n",
    "                    likely_words.append(word)\n",
    "                # deal with those words might have equally likely to follow the n-grams\n",
    "                elif temp_prob == probability_of_words:\n",
    "                    likely_words.append(word)\n",
    "            # return the set of words that are most likely to follow the n-gram\n",
    "            return set(likely_words)\n",
    "    \n",
    "    def generate_text(self, ngram: Tuple[str, ...], k: int) -> List[str]:\n",
    "        '''Generates a text sequence of length k, given a starting sequence.\n",
    "        :param ngram: starting sequence\n",
    "        :param k: total number of words in the generated sequence\n",
    "        :return: sequence of generated words, including the starting sequence\n",
    "        '''\n",
    "        # initial the list to store sequence of generated words\n",
    "        sequence_words = []\n",
    "        # start the looping to generate the word sequentially \n",
    "        for sequence in range (0, k):\n",
    "            # convert the most likely word that belong to the according ngram into list for easily manupulating\n",
    "            generated_token = list(self.most_likely_word(ngram))\n",
    "            # randomly select a token from the generated token list \n",
    "            random_token = random.choice(generated_token)\n",
    "            # append the first token into the sequence word from the first element of the generated token\n",
    "            sequence_words.append(random_token)\n",
    "            # update the ngram to predict the upcoming words until k\n",
    "            ngram = tuple([ngram[1:], random_token])\n",
    "        # return the list\n",
    "        return sequence_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f8ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T15:04:50.443075Z",
     "start_time": "2023-10-15T15:04:49.654025Z"
    }
   },
   "source": [
    "__e) Apply the model to a corpus (2 pts)__\n",
    "\n",
    "Finally, we want to apply our functions to the King James Bible (`'bible-kjv.txt'`) that is part of the ``gutenberg`` corpus. Use the function from HA 1, task 2a) to obtain a list of valid tokens (do not remove stopwords) from the King Jame Bible.\n",
    "\n",
    "Initialize the MarkovModel with the list of valid tokens and ``context_len=3`` and answer the following subtasks:\n",
    "\n",
    "i) What is the probability that the word ``babylon`` follows the sequence ``the king of``? \n",
    "\n",
    "ii) What are the most likely words to follow the sequence ``the world is``? \n",
    "\n",
    "iii) Generate a sequence of length 20 that starts with ``mother mary was``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4fe59750734a4a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:11.684457Z",
     "start_time": "2023-10-20T19:46:11.609558Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
    "\n",
    "def get_valid_tokens(tokens: Union[List[str], StreamBackedCorpusView]) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens that should be cleaned\n",
    "    :return: list of valid tokens\n",
    "    \"\"\"\n",
    "    # Remove punctuation mark and entirely numeric digits, and convert to lower case\n",
    "    valid_token = [i.lower() for i in tokens if i not in string.punctuation and not re.fullmatch(r'\\d+', i)]\n",
    "    return valid_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf82425c9dad366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:15.119632Z",
     "start_time": "2023-10-20T19:46:11.614070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/ngkokteng/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result for question i : 0.1906779661016949\n",
      "The result for question ii : {'enmity', 'gone', 'mine', 'the', 'crucified'}\n",
      "The result for question iii : ['espoused', 'persis', 'mint', 'midst', 'sharp', 'served', 'slanderously', 'hararite', 'descend', 'hadlai', 'moriah', 'bethphelet', 'sodden', 'rush', 'reward', 'foundations', 'ephod', 'belteshazzar', 'tasteth', 'edify']\n"
     ]
    }
   ],
   "source": [
    "# Download datasets and corpora\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "tokens = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
    "valid_tokens = get_valid_tokens(tokens)\n",
    "\n",
    "markov_model = MarkovModel(valid_tokens, 3)\n",
    "\n",
    "result_i = markov_model.transition_prob((\"the\", \"king\", \"of\"), \"babylon\")\n",
    "print(f\"The result for question i : {result_i}\")\n",
    "\n",
    "result_ii = markov_model.most_likely_word((\"the\", \"world\", \"is\"))\n",
    "print(f\"The result for question ii : {result_ii}\")\n",
    "\n",
    "result_iii = markov_model.generate_text((\"mother\", \"mary\", \"was\"), 20)\n",
    "print(f\"The result for question iii : {result_iii}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# hyewon code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5e3fbdde5f45b71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "\n",
    "class MarkovModel():\n",
    "    '''Markov model for generating text.'''\n",
    "    \n",
    "    def __init__(self, tokens: Sequence[str], context_len: int):\n",
    "        '''\n",
    "        :param tokens: text corpus on which the model is trained on as an iterator of tokens\n",
    "        :param context_len: length of the n-gram (number of preceding words)\n",
    "        '''\n",
    "        self.context_len = context_len \n",
    "        self.counts, self.v = self.process_corpus(tokens)\n",
    "        \n",
    "    def process_corpus(self, tokens: Sequence[str]) -> (Dict[Tuple[str, ...], Dict[str, int]], Set):\n",
    "        '''Training method of the model, counts the occurences of each word after each observed n-gram.\n",
    "        :param tokens: text corpus on which the model is trained on as an iterator of tokens\n",
    "        :returns: \n",
    "            - nested dict that maps each n-gram to the counts of the words succeeding it\n",
    "            - the whole vocabulary as a set\n",
    "        '''\n",
    "         # Initialize the n-gram counts and vocabulary\n",
    "        ngram_counts = defaultdict(Counter)\n",
    "        vocabulary = set()\n",
    "        \n",
    "        # Iterate through the tokens to collect n-gram counts\n",
    "        for ngram in ngrams(tokens, self.context_len + 1, pad_left=False, pad_right=False):\n",
    "            context = tuple(ngram[:-1])\n",
    "            word = ngram[-1]\n",
    "            ngram_counts[context][word] += 1\n",
    "            vocabulary.add(word)\n",
    "        \n",
    "        return ngram_counts, vocabulary\n",
    "        \n",
    "    def transition_prob(self, ngram: Tuple[str, ...], word: str) -> float:\n",
    "        '''Compute the conditional probability that the input word follows the given n-gram.\n",
    "        :param ngram: string tuple that represents an n-gram\n",
    "        :param word: input word\n",
    "        :return: probability that the n-gram is followed by the input word\n",
    "        '''\n",
    "       # Calculate the count of the n-gram followed by the word\n",
    "        ngram_count = self.counts.get(ngram, Counter())\n",
    "        count_word = ngram_count[word]\n",
    "        \n",
    "        # Calculate the sum of counts for all possible words in the vocabulary\n",
    "        total_count = sum(ngram_count.values())\n",
    "        total_vocabulary = len(self.v)\n",
    "        \n",
    "        # Compute the conditional probability\n",
    "        if total_count == 0:\n",
    "            return 1 / total_vocabulary  # If the n-gram is unseen, return uniform probability\n",
    "        else:\n",
    "            return count_word / total_count\n",
    "    \n",
    "    def most_likely_word(self, ngram: Tuple[str, ...]) -> Set[str]:\n",
    "        '''Computes which word is most likely to follow a given n-gram.\n",
    "        :param ngram: n-gram we are interested in\n",
    "        return: set of words that are most likely to follow the n-gram\n",
    "        '''\n",
    "       # Deal with the assumption for zero frequency of the n-gram (returning the entire vocabulary)\n",
    "        if ngram not in self.counts:\n",
    "            return set(self.v)\n",
    "    \n",
    "        else:\n",
    "        # Get the n-gram counts\n",
    "            ngram_count = self.counts[ngram]\n",
    "        \n",
    "        # Find the maximum count (the most likely count)\n",
    "            max_count = max(ngram_count.values(), default=0)\n",
    "        \n",
    "        # Use a list comprehension to find all words with the maximum count (the most likely words)\n",
    "            most_likely_words = {word for word, count in ngram_count.items() if count == max_count}\n",
    "        \n",
    "        return most_likely_words\n",
    "    \n",
    "    def generate_text(self, ngram: Tuple[str, ...], k: int) -> List[str]:\n",
    "        '''Generates a text sequence of length k, given a starting sequence.\n",
    "        :param ngram: starting sequence\n",
    "        :param k: total number of words in the generated sequence\n",
    "        :return: sequence of generated words, including the starting sequence\n",
    "        '''\n",
    "        sequence_words = list(ngram)\n",
    "\n",
    "        for _ in range(k - len(ngram)):\n",
    "            most_likely_words = self.most_likely_word(ngram)\n",
    "            next_word = random.choice(list(most_likely_words))\n",
    "            sequence_words.append(next_word)\n",
    "            ngram = ngram[1:] + (next_word,)\n",
    "\n",
    "        return sequence_words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be96067fc836da6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
    "\n",
    "def get_valid_tokens(tokens: Union[List[str], StreamBackedCorpusView], remove_stopwords: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens that should be cleaned\n",
    "    :return: list of valid tokens\n",
    "    \"\"\"\n",
    "    # Remove punctuation mark and entirely numeric digits, and convert to lower case\n",
    "    valid_token = [i.lower() for i in tokens if i not in string.punctuation and not re.fullmatch(r'\\d+', i)]\n",
    "    return valid_token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cc3d9099ae249dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download datasets and corpora\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "tokens = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
    "valid_tokens = get_valid_tokens(tokens)\n",
    "\n",
    "markov_model = MarkovModel(valid_tokens, 3)\n",
    "\n",
    "result_i = markov_model.transition_prob((\"the\", \"king\", \"of\"), \"babylon\")\n",
    "print(f\"The result for question i : {result_i}\")\n",
    "\n",
    "result_ii = markov_model.most_likely_word((\"the\", \"world\", \"is\"))\n",
    "print(f\"The result for question ii : {result_ii}\")\n",
    "\n",
    "result_iii = markov_model.generate_text((\"mother\", \"mary\", \"was\"), 20)\n",
    "print(f\"The result for question iii : {result_iii}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b8f15e4464cb236"
  },
  {
   "cell_type": "markdown",
   "id": "458bd11a5f702952",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb48b394f73a35",
   "metadata": {},
   "source": [
    "References for Task 3\n",
    "\n",
    "* https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk\n",
    "* https://sourajit16-02-93.medium.com/language-model-for-nlp-2fb56d4944aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T19:46:15.121124Z",
     "start_time": "2023-10-20T19:46:15.116163Z"
    }
   },
   "id": "3e7dbb4394f0da25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
