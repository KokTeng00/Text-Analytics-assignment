{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 23/24\n",
    "\n",
    "# Home Assignment 4 (38pts)\n",
    "\n",
    "**There is a shortcut to get the 17pts from Task 2 without implementing Task 1, see Task 2e)**\n",
    "\n",
    "\n",
    "Submit your solution via Ilias until 23.59h on Friday, November 24th. Late submissions are **not possible**.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
    "\n",
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions. \n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Team members and studentIDs:\n",
    "* Kok Teng Ng, 1936360\n",
    "* I-Chen Hsieh, 1870180\n",
    "* Hyewon Kang, 1980634\n",
    "* Minjeong Lee, 1978925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional packages (if any):\n",
    " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:31.219542Z",
     "start_time": "2023-11-19T19:23:30.874042Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import List, Union, Dict, Set, Tuple\n",
    "from numpy.typing import NDArray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Term Frequency - Inverse Document Frequency (21 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we want to use the term frequency - inverse document frequency (tf-idf) weighting method to compare documents with each other and to queries. In the end, we will apply our method to a subset of wikipedia pages (more specifically: only the introduction sections) that are linked to from the English Wikipedia page of Mannheim.\n",
    "\n",
    "In case you need to tokenize any sentences in the following tasks, please use a tokenizer from NLTK and not the ``string.split`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ To test your implementation throughout this task, you are given the example from exercise 8. Start by implementing a function ``process_docs`` that takes the provided dictionary of documents and returns the following data structures. __(4 pts)__\n",
    "\n",
    "- ``word2index``: a dictionary that maps each word that appears in any document to a unique integer identifier starting at 0 \n",
    "- ``doc2index``: a dictionary that maps each document name (here given as the dictionary keys) to a unique integer identifier starting at 0\n",
    "- ``index2doc``: a dictionary that maps each document identifier to the corresponding document name (reverse to ``doc2index``)\n",
    "- ``doc_word_vectors``: a dictionary that maps each document name to a list of word ids that indicate which words appeared in the document in their order of appearance. Words that appear multiple times must also be included multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:31.226440Z",
     "start_time": "2023-11-19T19:23:31.193950Z"
    }
   },
   "outputs": [],
   "source": [
    "# example from exercise 8\n",
    "d1 = \"cold beer beach\"\n",
    "d2 = \"ice cream beer beer\"\n",
    "d3 = \"beach cold ice cream\"\n",
    "d4 = \"cold beer frozen yogurt frozen beer\"\n",
    "d5 = \"frozen ice ice beer ice cream\"\n",
    "d6 = \"yogurt ice cream ice cream\"\n",
    "\n",
    "docs = {\"d1\": d1, \"d2\": d2, \"d3\": d3, \"d4\": d4, \"d5\": d5, \"d6\": d6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.305064Z",
     "start_time": "2023-11-19T19:23:31.201543Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def process_docs(docs: Dict[str, str]) -> (Dict[str, int], Dict[str, int], Dict[int, str], Dict[str, List[int]]):\n",
    "    \"\"\"\n",
    "    :params docs: dict that maps each document name to the document content\n",
    "    :returns:\n",
    "        - word2index: dict that maps each word to a unique id\n",
    "        - doc2index: dict that maps each document name to a unique id\n",
    "        - index2doc: dict that maps ids to their associated document name\n",
    "        - doc_word_vectors: dict that maps each document name to a list of word ids that appear in it\n",
    "    \"\"\"\n",
    "    # define the return variables for storing those needed information or token\n",
    "    word2index = dict({})\n",
    "    doc2index = dict({})\n",
    "    index2doc = dict({})\n",
    "    doc_word_vectors = dict({})\n",
    "    \n",
    "    # define the counter for handling the problem of word2index\n",
    "    token_counter = 0\n",
    "    # define the counter for handling the problem of doc2index and index2doc\n",
    "    doc_counter = 0\n",
    "    \n",
    "    # loop through the given documents (dictionary) and getting their keys and values\n",
    "    for key, value in docs.items():\n",
    "        # tokenize them by using nltk package\n",
    "        tokens = word_tokenize(value)\n",
    "        #loop through the tokenized list\n",
    "        for token in tokens:\n",
    "            # if the token not in the word2index then append them\n",
    "            if token not in word2index:\n",
    "                # append it into the word2index\n",
    "                word2index[token] = token_counter\n",
    "                # add one for making it become sequentially\n",
    "                token_counter = token_counter + 1\n",
    "        # adding the document index into the doc2index dictionary\n",
    "        doc2index[key] = doc_counter\n",
    "        index2doc[doc_counter] = key\n",
    "        # adding one to the counter to making it to handling the sequentially problem\n",
    "        doc_counter = doc_counter + 1\n",
    "    \n",
    "    # the reason can't do the doc_word_vectors at above code is the word2index hasn't completely defined so it might have problem when execute at above code\n",
    "    # loop through the given documents again\n",
    "    for key, value in docs.items():\n",
    "        # tokenize them by using nltk package\n",
    "        tokens = word_tokenize(value)\n",
    "        # define the temp index list to handle the token's ids\n",
    "        temp_index = []\n",
    "        # loop through the tokenized list of the current document\n",
    "        for token in tokens:\n",
    "            # adding their ids into the temp index list by referring to the word2index (for getting their ids)\n",
    "            temp_index.append(word2index[token])\n",
    "        # adding the temp_index list into the doc_word_vector dictionary\n",
    "        doc_word_vectors[key] = temp_index\n",
    "        \n",
    "    # return required result\n",
    "    return word2index, doc2index, index2doc, doc_word_vectors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.313979Z",
     "start_time": "2023-11-19T19:23:33.295388Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
      "{'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
      "{0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
      "{'d1': [0, 1, 2], 'd2': [3, 4, 1, 1], 'd3': [2, 0, 3, 4], 'd4': [0, 1, 5, 6, 5, 1], 'd5': [5, 3, 3, 1, 3, 4], 'd6': [6, 3, 4, 3, 4]}\n"
     ]
    }
   ],
   "source": [
    "word2index, doc2index, index2doc, doc_word_vectors = process_docs(docs)\n",
    "print(word2index)\n",
    "print(doc2index)\n",
    "print(index2doc)\n",
    "print(doc_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.314546Z",
     "start_time": "2023-11-19T19:23:33.304141Z"
    }
   },
   "outputs": [],
   "source": [
    "# The output for the provided example could look like this:\n",
    "\n",
    "# word2index:\n",
    "# {'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
    "\n",
    "# doc2index:\n",
    "# {'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
    "\n",
    "# index2doc\n",
    "# {0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
    "\n",
    "# doc_word_vectors:\n",
    "# {'d1': [0, 1, 2],\n",
    "#  'd2': [3, 4, 1, 1],\n",
    "#  'd3': [2, 0, 3, 4],\n",
    "#  'd4': [0, 1, 5, 6, 5, 1],\n",
    "#  'd5': [5, 3, 3, 1, 3, 4],\n",
    "#  'd6': [6, 3, 4, 3, 4]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b)__ Set up a term-document matrix where each column corresponds to a document and each row corresponds to a word that was observed in any of the documents. The row/column indices should correspond to the word/document ids that are set in the input dicts ``word2index`` and ``doc2index``. Count how often each word appears in each document and fill the term document matrix. __(3 pts)__\n",
    "\n",
    "_Example: The word \"beer\" with the word id_ ``1`` _appears two times in the document \"d4\" that has the document id_ ``3``. _Therefore the the entry at position_ ``[1, 3]`` _in the term-document matrix is_ ``2``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.320128Z",
     "start_time": "2023-11-19T19:23:33.309475Z"
    }
   },
   "outputs": [],
   "source": [
    "def term_document_matrix(doc_word_v: Dict[str, List[int]], doc2index: Dict[str, int], word2index: Dict[str, int]) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param doc_word_v: dict that maps each document to the list of word ids that appear in it\n",
    "    :param doc2index: dict that maps each document name to a unique id\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :return: term-document matrix (each word is a row, each document is a column) that indicates the count of each word in each document \n",
    "    \"\"\"\n",
    "    # getting the length size of document and the vocabulary length\n",
    "    doc_len = len(doc2index)\n",
    "    token_len = len(word2index)\n",
    "    \n",
    "    # define the matrix for handling the term frequency (as defining zero to all of them)\n",
    "    term_freq = np.zeros((token_len, doc_len))\n",
    "\n",
    "    counter = 0\n",
    "    for ids in doc_word_v.values():\n",
    "        for id in ids:\n",
    "            term_freq[id, counter] = term_freq[id, counter] + 1\n",
    "        counter = counter + 1\n",
    "    \n",
    "    return term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.335632Z",
     "start_time": "2023-11-19T19:23:33.316306Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 1., 0., 0.],\n",
       "       [1., 2., 0., 2., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 3., 2.],\n",
       "       [0., 1., 1., 0., 1., 2.],\n",
       "       [0., 0., 0., 2., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
    "term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c)__ Implement the function ``to_tf_idf_matrix`` that takes a term-document matrix and returns the corresponding term frequency (tf) matrix. If the parameter ``idf`` is set to ``True``, the tf-matrix should further be transformed to a tf-idf matrix (i.e. every entry corresponds to the tf-idf value of the associated word and document). Your implementation should leave the input term-document matrix unchanged. __(3 pts)__\n",
    "\n",
    "Recall the following formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "  tf_{t,d} =\n",
    "    \\begin{cases}\n",
    "      1+log_{10}\\text{count}(t,d) & \\text{if count}(t, d) > 0\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}       \n",
    "\\end{equation}  \n",
    "\n",
    "$$idf_t = log_{10}(\\frac{N}{df_i})$$  \n",
    "\n",
    "$$tf-idf_{t,d} = tf_{t,d} \\cdot idf_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.342517Z",
     "start_time": "2023-11-19T19:23:33.322720Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def to_tf_idf_matrix(td_matrix: NDArray[NDArray[float]], idf: bool=True) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param td_matrix: term-document matrix \n",
    "    :param idf: computes the tf-idf matrix if True, otherwise computes only the tf matrix\n",
    "    :return: matrix with tf(-idf) values for each word-document pair \n",
    "    \"\"\"\n",
    "    # define the total number of document and vocabulary\n",
    "    total_doc = len(td_matrix[0])\n",
    "    total_word = len(td_matrix)\n",
    "    \n",
    "    # make a deep copy for the td matrix for computing the term frequency\n",
    "    tf_matrix = copy.deepcopy(td_matrix)\n",
    "    \n",
    "    # process of computing the value of term frequency\n",
    "    for column in range(total_doc):\n",
    "        # compute the length of those documents\n",
    "        doc_word_counts = np.sum(tf_matrix[:, column])\n",
    "        # loop through each cell of each column to compute their tf\n",
    "        for word in range (total_word):\n",
    "            # if the cell is not equal to zero then use the given formula to compute\n",
    "            if tf_matrix[word, column] != 0:\n",
    "                tf_matrix[word, column] = 1 + np.log10(tf_matrix[word, column] / doc_word_counts)\n",
    "            # else assign zero to them\n",
    "            else:\n",
    "                tf_matrix[word, column] = 0\n",
    "    \n",
    "    # the process of computing the inverse document frequency for them when idf is defined as True\n",
    "    if idf:\n",
    "        # getting the number of document that appear certain word\n",
    "        doc_freq = np.count_nonzero(td_matrix, axis = 1)\n",
    "        # use the given formula to compute the idf matrix for each word\n",
    "        idf_matrix = np.log10(total_doc/ doc_freq)\n",
    "        # multiply the tf-matrix with the idf-matrix to get the result of tf-idf\n",
    "        tf_idf_matrix = tf_matrix * idf_matrix[:, np.newaxis]\n",
    "        # return the tf-idf matrix\n",
    "        return tf_idf_matrix\n",
    "    \n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.343132Z",
     "start_time": "2023-11-19T19:23:33.327689Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15740219, 0.        , 0.11979188, 0.06678313, 0.        ,\n",
       "        0.        ],\n",
       "       [0.09207438, 0.12308251, 0.        , 0.09207438, 0.03906563,\n",
       "        0.        ],\n",
       "       [0.24947656, 0.        , 0.18986564, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.07007376, 0.07007376, 0.        , 0.12308251,\n",
       "        0.1060175 ],\n",
       "       [0.        , 0.07007376, 0.07007376, 0.        , 0.03906563,\n",
       "        0.1060175 ],\n",
       "       [0.        , 0.        , 0.        , 0.24947656, 0.10584875,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.10584875, 0.        ,\n",
       "        0.14362781]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix = to_tf_idf_matrix(term_matrix, True)\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ We want to test our implementation on our running example. First, print the tf-idf for each word of the query ``ice beer`` with respect to each document. Second, find the two most similar documents from ``d1, d2, d3`` according to cosine similarity and print all similarity values.  __(3 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.543334Z",
     "start_time": "2023-11-19T19:23:33.332201Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for 'ice' and 'beer':\n",
      "[[[0.         0.07007376 0.07007376 0.         0.12308251 0.1060175 ]]\n",
      "\n",
      " [[0.09207438 0.12308251 0.         0.09207438 0.03906563 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# the first sub-question as printing the tf-idf for each word of the query ice beer with respect to each document\n",
    "# define the required query token\n",
    "query_words = [\"ice\", \"beer\"]\n",
    "\n",
    "# initialized a list for storing the query tf-idf value\n",
    "query_tf_idf = []\n",
    "\n",
    "# loop through the query word list\n",
    "for word in query_words:\n",
    "    # initialized a temp list to store the tf-idf value for each query token\n",
    "    temp = []\n",
    "    # check whether the query token stored appear in the training corpus or not through the word2index dictionary, if yes extract their tf-idf values within each document, else pass\n",
    "    if word in word2index:\n",
    "        # append the query token tf-idf value from each document\n",
    "        temp.append(tf_idf_matrix[word2index[word]])\n",
    "    # append the query token to the query_tf_idf\n",
    "    query_tf_idf.append(temp)\n",
    "\n",
    "print(\"TF-IDF for 'ice' and 'beer':\")\n",
    "print(np.array(query_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.545919Z",
     "start_time": "2023-11-19T19:23:33.339615Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between doc1 and doc2: 0.2320830070172335\n",
      "Cosine Similarity between doc1 and doc3: 0.8732802004950572\n",
      "Cosine Similarity between doc2 and doc3: 0.2532575881165856\n"
     ]
    }
   ],
   "source": [
    "# the second sub-question as finding the two most similar documents from d1, d2, d3 according to cosine similarity and print all similarity values\n",
    "# define the cosine similarity function\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    # using the dot product to multiply two matrix \n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    # using the L2 norm method for both matrix to compute their norm values (distance)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    # divide them will get the cosine similarity and return\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "similarity_doc1_doc2 = cosine_similarity(tf_idf_matrix[:, 0], tf_idf_matrix[:, 1])\n",
    "similarity_doc1_doc3 = cosine_similarity(tf_idf_matrix[:, 0], tf_idf_matrix[:, 2])\n",
    "similarity_doc2_doc3 = cosine_similarity(tf_idf_matrix[:, 1], tf_idf_matrix[:, 2])\n",
    "print(f\"Cosine Similarity between doc1 and doc2: {similarity_doc1_doc2}\")\n",
    "print(f\"Cosine Similarity between doc1 and doc3: {similarity_doc1_doc3}\")\n",
    "print(f\"Cosine Similarity between doc2 and doc3: {similarity_doc2_doc3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ In a second step we want to find the documents that are most similar to a provided query. Therefore, implement the function ``process_query`` that creates a vector represention of the query. __(5 pts)__\n",
    "\n",
    "Create a vector that has an entry for each vocabulary word (words that appeared in any document), where the entry at position ``i`` indicates how often the word with id ``i`` (as indicated by ``word2index``) appears in the query. \n",
    "\n",
    "If ``tf`` is set to ``True``, you should transform all entries to tf-values. Similar, if ``idf`` is set to ``True``, return a vector with tf-idf values (cf. task __c)__). The computation of the idf values is based on the corresponding input term-document matrix.\n",
    "\n",
    "In case the query contains words that are in any of the documents, print an appropriate error message and stop the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.570714Z",
     "start_time": "2023-11-19T19:23:33.342743Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_query(query: List[str], word2index: Dict[str, int], td_matrix: NDArray[NDArray[float]], tf: bool=True, idf: bool=True) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    :param query: list of query tokens\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :param td_matrix: term-document matrix\n",
    "    :param tf: computes the tf vector of the query if True\n",
    "    :param idf: computes the tf-idf vector of the query if True, ignored if tf=False\n",
    "    :return: vector representation of the input query (using tf(-idf))    \n",
    "    \"\"\"\n",
    "    # initialize a vector for the query with the length of the vocabulary\n",
    "    query_vector = np.zeros(len(word2index))\n",
    "\n",
    "    # loop through the given query token list \n",
    "    for word in query:\n",
    "        # for adding the given query token frequency within the query vector when the query token has appeared in the given tf-idf before\n",
    "        if word in word2index:\n",
    "            # adding frequency\n",
    "            query_vector[word2index[word]] = query_vector[word2index[word]] + 1\n",
    "\n",
    "    \n",
    "    # for computing the tf value when it is defined as true\n",
    "    if tf == True:\n",
    "        # compute the vocabulary within the given tf-idf\n",
    "        total_words = sum(query_vector)\n",
    "        # loop through the query vector\n",
    "        for i in range (len(query_vector)):\n",
    "            # if the cell is not equal to zero then compute their tf value\n",
    "            if query_vector[i] != 0:\n",
    "                # using the given tf formula to compute the value\n",
    "                query_vector[i] = 1 + np.log10(query_vector[i] / total_words)\n",
    "            # else assign zero to it\n",
    "            else:\n",
    "                query_vector[i] = 0\n",
    "    \n",
    "    # for computing the tf-idf value when idf and tf are defined as True            \n",
    "    if (idf == True) and (tf == True):\n",
    "        # getting the number of document that appear certain word\n",
    "        doc_freq = np.count_nonzero(td_matrix, axis = 1)\n",
    "        # using the given formula to compute the idf value\n",
    "        idf_matrix = np.log10(len(td_matrix[0]) / doc_freq)\n",
    "        # multiply the query vector (that is storing the tf value) and the idf vector to getting the final answer of tf-idf value\n",
    "        query_vector = query_vector * idf_matrix[np.newaxis]\n",
    "        \n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.571330Z",
     "start_time": "2023-11-19T19:23:33.350279Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.12308251 0.         0.12308251 0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "query = [\"ice\", \"beer\"]\n",
    "query_vector = process_query(query, word2index, term_matrix, tf=True, idf=True)\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f)__ Implement a function ``most_similar_docs`` that gets the vector representation of a query (in terms of counts, tf values or tf-idf values) and a term-document matrix that can either contain counts, tf-values or tf-idf values.  Compute the cosine similarity between the query and all documents and return the document names and the cosine similarity values of the top-``k`` documents that are most similar to the query. The value of ``k`` should be specified by the user. __(3 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.571464Z",
     "start_time": "2023-11-19T19:23:33.356261Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar_docs(query_v: NDArray[float], td_matrix: NDArray[NDArray[float]], index2doc: Dict[int, str], k: int) -> (List[str], List[float]):\n",
    "    \"\"\"\n",
    "    :param query_v: vector representation of the input query\n",
    "    :param td_matrix: term-document matrix, possibly with tf-(idf) values \n",
    "    :param index2doc: dict that maps each document id to its name\n",
    "    :k: number of documents to return\n",
    "    :returns:\n",
    "        - list with names of the top-k most similar documents to the query, ordered by descending similarity\n",
    "        - list with cosine similarities of the top-k most similar docs, ordered by descending similarity\n",
    "    \"\"\"\n",
    "    # initialize the similarity dictionary for storing the similarity between qeury vector with the given documents\n",
    "    similarity = {}\n",
    "    \n",
    "    # getting the total number of document from the original corpus\n",
    "    total_doc = len(td_matrix[0])\n",
    "    # loop through all the document in the tf-idf matrix\n",
    "    for column in range (total_doc):\n",
    "        # using the pre-defined cosine similarity function in question d to compute their similarity and store within the similarity dictionary\n",
    "        similarity[index2doc[column]] = cosine_similarity(query_v, td_matrix[:, column])\n",
    "    \n",
    "    # sorting the dictionary based on the similarity score\n",
    "    sorted_similarity = sorted(similarity.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # save the top k name and similarity score within a list for returning the result\n",
    "    top_k_names = [item[0] for item in sorted_similarity[:k]]\n",
    "    top_k_cos_similarities = [item[1] for item in sorted_similarity[:k]]\n",
    "\n",
    "    return top_k_names, top_k_cos_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.571907Z",
     "start_time": "2023-11-19T19:23:33.360517Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['d2', 'd5'], [array([0.86434041]), array([0.668625])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_docs(query_vector, tf_idf_matrix,index2doc,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Text Classification (17pts)\n",
    "In this task, we want to build a logistic regression classifier to classify 20newsgroups posts. As feature representation, we want to use tf-idf vectors as just implemented.\n",
    "\n",
    "### Logistic Regression\n",
    "Implement a logistic regression classifier, similar to exercise 7. Again, you don't need to add a bias weight/feature.\n",
    "\n",
    "__a)__ Implement the `predict_proba` function in the `LogisticRegression` class below. Your function should return the output of a logistic regression classifier according to the current assignments of weights $\\mathbf{w}$, i.e., \n",
    "$$\n",
    "expit(\\mathbf{w}^T\\mathbf{x})\n",
    "$$\n",
    "You can assume that model weights are stored in a variable `self.w`. __(3pts)__\n",
    "\n",
    "__b)__ Implement the `predict` function in the `LogisticRegression` class below. The prediction should return class `1` if the classifier output is above 0.5, otherwise `0` __(3pts)__\n",
    "\n",
    "__c)__ Implement the `fit` function to learn the model parameters `w` with stochastic gradient descent for one epoch, i.e., going over the training data once. Store the learned parameters in a variable `self.w`. Only initialize the parameters randomly in the first training iteration and continue with learned parameters in later iterations. Make sure, that you iterate over instances in each epoch randomly.  __(5pts)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:33.577215Z",
     "start_time": "2023-11-19T19:23:33.367802Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class LogisticRegression():\n",
    "    '''Logistic Regression Classifier.'''\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, x: NDArray[NDArray[float]], y: NDArray[int], eta: float=0.1, epochs: int = 1):\n",
    "        '''\n",
    "        :param x: 2D numpy array where each row is an instance\n",
    "        :param y: 1D numpy array with target classes for instances in x\n",
    "        :param eta: learning rate, default is 0.1\n",
    "        :param epochs: fixed number of epochs as stopping criterion, default is 10\n",
    "        '''\n",
    "        # c)\n",
    "        # set the first weights to random value\n",
    "        if self.w is None:\n",
    "            self.w = np.random.rand(x.shape[1])\n",
    "        \n",
    "        # perform stochastic gradient decent\n",
    "        for i in range(epochs):\n",
    "            # shuffle the data indices for each epoch\n",
    "            for j in np.random.permutation(len(y)):\n",
    "                # get the sample and label\n",
    "                xj, yj = x[j], y[j]\n",
    "                # predict probability for the sample\n",
    "                prediction = self.predict_proba(xj)\n",
    "                # compute gradient: (prediction - actual) * feature vector\n",
    "                gradient = (prediction - yj) * xj\n",
    "                # update weigths\n",
    "                self.w -= eta * gradient\n",
    "        \n",
    "    def predict_proba(self, x: NDArray) -> float:\n",
    "        # a)\n",
    "        # calculate logistic regression output using the sigmoid function\n",
    "        return expit(np.dot(self.w, x))\n",
    "        \n",
    "    def predict(self, x: NDArray) -> int:\n",
    "        # b)\n",
    "        # predict class, 1 if p > 0.5, otherwise 0\n",
    "        return 1 if self.predict_proba(x) > 0.5 else 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ Apply your model to the two categories 'comp.windows.x' and 'rec.motorcycles' from the 20newsgroups data. To this end, first transform the training data to tf-idf representation with the functions `process_docs`, `term_document_matrix` and `to_tfidf_matrix`. Transform the test documents with `process_query`. Fit your model on the training data for 10 epochs. Calculate the accuracy on the test data. __(6pts)__\n",
    "\n",
    "**Shortcut**: use the `TfidfVectorizer` from scikit learn (you may need to transform its output to a dense (array) representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:23:34.773172Z",
     "start_time": "2023-11-19T19:23:33.373173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9419924337957125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['comp.windows.x','rec.motorcycles'])\n",
    "test = fetch_20newsgroups(subset='test', categories=['comp.windows.x','rec.motorcycles'])\n",
    "categories = ['comp.windows.x', 'rec.motorcycles']\n",
    "\n",
    "# convert the data to tf-idf representation\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(train.data)\n",
    "x_test = vectorizer.transform(test.data)\n",
    "y_train = train.target\n",
    "y_test = test.target\n",
    "\n",
    "\n",
    "# initialize the logistic regression model\n",
    "logistic_r = LogisticRegression()\n",
    "# fit the model\n",
    "logistic_r.fit(x_train.toarray(), y_train, eta=0.1, epochs=10)\n",
    "# predictions on test data\n",
    "y_predict = np.array([logistic_r.predict(xi) for xi in x_test.toarray()])\n",
    "\n",
    "# calculate accuracy \n",
    "accuracy = np.mean(y_predict == y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regession with methods from task1\n",
    "from scipy.special import expit\n",
    "\n",
    "class LogisticRegression_2():\n",
    "    '''Logistic Regression Classifier.'''\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, x: NDArray[NDArray[float]], y: NDArray[int], eta: float=0.1, epochs: int=10):\n",
    "        '''\n",
    "        :param x: 2D numpy array where each row is an instance\n",
    "        :param y: 1D numpy array with target classes for instances in x\n",
    "        :param eta: learning rate, default is 0.1\n",
    "        :param epochs: fixed number of epochs as stopping criterion, default is 10\n",
    "        '''\n",
    "        # c)\n",
    "        num_points = x.shape[0] # number of instances\n",
    "        dim = x.shape[1] # number of features\n",
    "\n",
    "        if self.w == None:\n",
    "            # initialize the parameters randomly in range [0,1] in first iteration \n",
    "            self.w = np.random.rand(dim)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            idx = np.arange(num_points)\n",
    "            # iterate over instances randomly\n",
    "            np.random.shuffle(idx)\n",
    "            for i in idx:\n",
    "                predict = self.predict(x[i]) # predict the value of the chosen example based on the current hypothesis\n",
    "                error = predict - y[i]   # calculate the error\n",
    "                self.w = self.w - eta*error*x[i]   # update the weight vector\n",
    "\n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "        # a)\n",
    "        return expit(np.dot(x,self.w))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # b)\n",
    "        return np.where(self.predict_proba(x) < 0.5,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8146279949558638"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['comp.windows.x','rec.motorcycles'])\n",
    "test = fetch_20newsgroups(subset='test', categories=['comp.windows.x','rec.motorcycles'])\n",
    "        \n",
    "train_dict = {f\"d{i+1}\": train.data[i] for i in range(len(train.data))}\n",
    "word2index, doc2index, index2doc, doc_word_vectors = process_docs(train_dict)\n",
    "term_matrix = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
    "tf_idf_matrix = to_tf_idf_matrix(term_matrix, True)\n",
    "\n",
    "# training Logistic Regression\n",
    "clf = LogisticRegression_2()\n",
    "clf.fit(tf_idf_matrix.transpose(), train.target)\n",
    "\n",
    "# predict test data\n",
    "test_token = [word_tokenize(i) for i in test.data]\n",
    "test_query_vector = np.array([process_query(token, word2index, term_matrix, tf=True, idf=True) for token in test_token])\n",
    "pred = clf.predict(test_query_vector)\n",
    "accuracy_score(test.target,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### References\n",
    "\n",
    "References for Task 1\n",
    "* https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "* https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "\n",
    "References for Task 2\n",
    "* https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a\n",
    "* https://www.kaggle.com/code/shahkan/text-classification-using-logistic-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
