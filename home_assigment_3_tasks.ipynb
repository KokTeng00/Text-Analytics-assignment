{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080be2e1",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 23/24\n",
    "\n",
    "# Home Assignment 3 (30pts)\n",
    "\n",
    "Submit your solution via Ilias until 23.59h on Wednesday, November 8th. Late submissions are accepted until 12:00am on the following day, with 1/4 of the total possible points deducted from the score.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
    "\n",
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions. \n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Team members and studentIDs:\n",
    "* Kok Teng Ng, 1936360\n",
    "* I-Chen Hsieh, 1870180\n",
    "* Hyewon Kang, 1980634\n",
    "* Minjeong Lee, 1978925"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ace3d36b1b890070"
  },
  {
   "cell_type": "markdown",
   "id": "02ed9164",
   "metadata": {},
   "source": [
    "Additional packages (if any):\n",
    " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ef36f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:56.934235Z",
     "start_time": "2023-10-28T11:59:48.244242Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "from numpy.typing import NDArray\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc6b92",
   "metadata": {},
   "source": [
    "### Task 1: POS tagging (6 points)\n",
    "\n",
    "In this task, we want to explore sentences with similar part of speech (POS) tag structure. For this, we need a corpus of text with tags. We will generate such a corpus by using NLTKâ€™s currently recommended POS tagger to tag a given list of tokens (https://www.nltk.org/api/nltk.tag.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d59e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:57.022271Z",
     "start_time": "2023-10-28T11:59:56.920719Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK's off-the-shelf POS tagger\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78bb0d",
   "metadata": {},
   "source": [
    "__a)__ Given a corpus of text ``corpus`` as a sequence of tokens, we want to collect all words that are tagged with a certain POS tag. Implement a function ``collect_words_for_tag`` that first tags the given corpus using NLTK's off-the-shelf tagger imported in the cell above. Then, for each POS tag, collect all words that were tagged with it. You should return a dictionary that maps each POS tag that was observed to the set of words that were assigned this tag in the given corpus. __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5651149b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:57.025124Z",
     "start_time": "2023-10-28T11:59:56.930658Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
    "\n",
    "def collect_words_for_tag(corpus: Union[List[str], StreamBackedCorpusView]) -> Dict[str, Set[str]]:\n",
    "    '''\n",
    "    :param corpus: sequence of tokens that represents the text corpus\n",
    "    :return: dict that maps each tag to a set of tokens that were assigned this tag in the corpus\n",
    "    '''\n",
    "    # your code here\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba737321",
   "metadata": {},
   "source": [
    "__b)__ Implement a function ``generate_sentences`` that gets a sentence and a POS dictionary (assume the POS dictionary was generated by your function in __a)__) as input and generates ``n`` sequences of words with the same tag structure. The words in your generated sequence should be randomly taken from the set of words associated with the current tag. \n",
    "\n",
    "Additionally, the user should have the option to achieve sentences of ``better_quality``. Thus, if ``better_quality=True``, make sure that the tag structure of the output sentences actually matches the tag structure of the input sentence, as the tags may change depending on the context. \n",
    "\n",
    "You can assume that the training corpus is large enough to include all possible POS tags. __(2 pts)__\n",
    "\n",
    "_Hint: consider the_ ``random`` _module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4efad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:57.035192Z",
     "start_time": "2023-10-28T11:59:57.017723Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_rand(sentence: List[str], pos_dict: Dict[str, Set[str]], n: int, better_quality: bool=False) -> List[List[str]]:\n",
    "    '''\n",
    "    :param sentence: input sentence that sets the tag pattern\n",
    "    :param pos_dict: maps each tag to a list of associated words\n",
    "    :param n: number of sentences that should be generated\n",
    "    :return: List of sentences with the same tag structure as the input sentence\n",
    "    '''\n",
    "    # your code here\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b3ba",
   "metadata": {},
   "source": [
    "__c)__ Using the input sentence ``This test is very difficult``, test your implementation to generate 10 sentences based on  \n",
    "\n",
    "* \"Emma\" by Jane Austen\n",
    "\n",
    "* The \"King James Bible\"\n",
    "\n",
    "Store your POS dictionary in ``emma_tags``and ``bible_tags``, respectively. Your generated sentences should be stored in ``emma_sent`` and ``bible_sent``. __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69ab118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:57.036428Z",
     "start_time": "2023-10-28T11:59:57.021832Z"
    }
   },
   "outputs": [],
   "source": [
    "sent = [\"This\", \"test\", \"is\", \"very\", \"difficult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad042eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:57.058977Z",
     "start_time": "2023-10-28T11:59:57.023423Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecad4e",
   "metadata": {},
   "source": [
    "### Task 2: The Viterbi algorithm (12 points)\n",
    "Implement the Viterbi algorithm as introduced in the lecture (lecture 8, slide 20) and the exercise. The input of your function is a sentence that should be tagged, a dictionary with state transition probabilites and a dictionary with word emission probabilities. You may assume that the _transition probabilities_ are complete, i.e. the dictionary includes every combination of states. In contrast, we assume that all combinations of words and POS tags that are not in the dictionary of _emission probabilities_ have an emission probability of 0.\n",
    "\n",
    "The function should return a list of POS tags, s.t. that each tag corresponds to a word of the input sentence. Moreover, return the probability of the sequence of POS tags that you found. \n",
    "\n",
    "You can test your function on the given example that was discussed in the Pen&Paper exercise. For the sentence ``the fans watch the race`` and the provided probabilities, your function should return the POS tag sequence ``['DT', 'N', 'V', 'DT', 'N']`` and a probability of ``9.720000000000002e-06``.\n",
    "\n",
    "Additionally, implement beam search in the viterbi algorithm. The beam size is defined by the parameter `beam`. For example for `beam=2` we only keep the best 2 scores per column in each step and discard the rest. You may use the example from the lecture to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8319309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:57.060224Z",
     "start_time": "2023-10-28T11:59:57.024415Z"
    }
   },
   "outputs": [],
   "source": [
    "# test sentence\n",
    "sentence = [\"the\", \"fans\", \"watch\", \"the\", \"race\"]\n",
    "\n",
    "# state transition probabilities (complete)\n",
    "state_trans_prob = {('<s>','DT'):0.8,('<s>','N'):0.2,('<s>','V'):0.0,\n",
    "                    ('DT','DT'):0.0,('DT','N'):0.9,('DT','V'):0.1,\n",
    "                    ('N','DT'):0.0,('N','N'):0.5,('N','V'):0.5,\n",
    "                    ('V','DT'):0.5,('V','N'):0.5,('V','V'):0.0}\n",
    "\n",
    "# word emission probabilities (not complete, all combinations that are not present have probability 0)\n",
    "word_emission_prob = {('the','DT'):0.2, ('fans','N'):0.1,('fans','V'):0.2,('watch','N'):0.3,\n",
    "                      ('watch','V'):0.15,('race','N'):0.1,('race','V'):0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9e4d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:59:57.061278Z",
     "start_time": "2023-10-28T11:59:57.025500Z"
    }
   },
   "outputs": [],
   "source": [
    "def Viterbi(sentence: List[str], trans_prob: Dict[Tuple[str,str], float], emiss_prob: Dict[Tuple[str,str], float], beam: int=0) -> (List[str], float):\n",
    "    '''\n",
    "    :param sentence: sentence that we want to tag\n",
    "    :param trans_prob: dict with state transition probabilities\n",
    "    :param emiss_prob: dict with word emission probabilities\n",
    "    :param beam: beam size for beam search. If 0, don't apply beam search\n",
    "    :returns: \n",
    "        - list with POS tags for each input word\n",
    "        - float that indicates the probability of the tag sequence\n",
    "    '''\n",
    "    # your code here\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c38fe7",
   "metadata": {},
   "source": [
    "### Task 3: ML Basics - Naive Bayes Classification (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ab66f",
   "metadata": {},
   "source": [
    "### Task 3: ML Basics - Naive Bayes Classification (12pts)\n",
    "In this task, we want to build a Naive Bayes classifier with add-1 smoothing for text classification (pseudocode given below), e.g., to assign a category to a document. Use the class-skeleton provided below for your implementation.\n",
    "\n",
    "#### Naive Bayes Pseudocode\n",
    "##### TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$)  \n",
    "$V \\leftarrow extractVocabulary(\\mathbb D)$  \n",
    "$N \\leftarrow countDocs(\\mathbb D)$    \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N_c \\leftarrow countDocsInClass(\\mathbb D, c)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$prior[c] \\leftarrow \\frac{N_c}{N}$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$text_c \\leftarrow concatenateTextOfAllDocsInClass(\\mathbb D, c)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T_{ct} \\leftarrow countTokensOfTerm(text_c,t)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$condprob[t][c] \\leftarrow \\frac{T_{ct} + 1}{\\sum_{t'}(T_{ct'} + 1)}$  \n",
    "return $V,prior,condprob$\n",
    "\n",
    "##### ApplyMultinomialNB($\\mathbb C,V,prior,condprob,d$)\n",
    "$W \\leftarrow extractTokensFromDoc(V,d)$   \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$score[c] \\leftarrow log(prior[c])$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in W$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$score[c] += log(condprob[t][c])$  \n",
    "return $argmax_{c \\in \\mathbb C} score[c]$\n",
    "\n",
    "__a) Tokenization (1pt)__  \n",
    "Implement the function `tokenize` to transform a text document to a list of tokens with the regex pattern `\\b\\w\\w+\\b`. Transform all tokens to lowercase.\n",
    "\n",
    "__b) Naive Bayes \"Training\" (6pts)__  \n",
    "Implement the `__init__` function to set up the Naive Bayes Model. Cf. TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$) in the pseudocode above. Contrary to the pseudocode, the `__init__` function should not return anything, but the vocabulary, priors and conditionals should be stored in class variables. We only want to keep tokens with a frequeny > `min_count` in the vocabulary.\n",
    "\n",
    "__c) Naive Bayes Classification (3pts)__  \n",
    "Implement the `classify` function to return the most probable class for the provided document according to your Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    '''Naive Bayes for text classification.'''\n",
    "    def __init__(self, docs: List[str], labels: List[int], min_count: int=1):\n",
    "        '''\n",
    "        :param docs: list of documents from which to build the model (corpus)\n",
    "        :param labels: list of classes assigned to the list of documents (labels[i] is the class for docs[i])\n",
    "        :param min_count: minimum frequency of token in vocabulary (tokens that occur less times are discarded)\n",
    "        '''\n",
    "        # generate the vocabulary of the given document\n",
    "        # utilise the defined tokenize function to tokenize given documents\n",
    "        tokenized_docs = [self.tokenize(doc) for doc in docs]\n",
    "        # define the list to contain all tokens \n",
    "        all_tokens = []\n",
    "        # go through the tokenized document \n",
    "        for tokens in tokenized_docs:\n",
    "            # loop through the token\n",
    "            for token in tokens:\n",
    "                # append the token into the all_tokens variable\n",
    "                all_tokens.append(token)\n",
    "        # getting the frequency of each token within the given documents\n",
    "        token_counts = Counter(all_tokens)\n",
    "        # initialize the vocabulary list\n",
    "        self.vocabulary = []\n",
    "        # loop through the entire counter after tokenized the given documents, and get their word and frequency\n",
    "        for word, frequency in token_counts.items():\n",
    "            # doing logic operation whether the token's frequency greater than the minimum count, if more than append into vocabulary else next word\n",
    "            if frequency > min_count:\n",
    "                self.vocabulary.append(word)\n",
    "\n",
    "        # getting the length of given documents\n",
    "        self.count_docs = len(labels)\n",
    "        \n",
    "        # getting the prior probability\n",
    "        # generate the counter for given label of the documents\n",
    "        self.prior_probability = Counter(labels)\n",
    "        # loop through the entire counter for computing the prior probability \n",
    "        for label, frequency in self.prior_probability.items():\n",
    "            # use the frequency of label divided by the count documents \n",
    "            self.prior_probability[label] = frequency / len(self.vocabulary)\n",
    "        \n",
    "        # concatenate all given documents with its own class \n",
    "        # define the dictionary for storing the concatenation of document and class\n",
    "        text_class = {}\n",
    "        for uni_label in set(labels):\n",
    "            # define the list within the dictionary for each label\n",
    "            text_class[uni_label] = []\n",
    "        for no_sentence, label in enumerate(labels):\n",
    "            # generate the token for each document\n",
    "            token = self.tokenize(docs[no_sentence])\n",
    "            # concatenate the document into class\n",
    "            text_class[label] = text_class[label] + token\n",
    "                    \n",
    "        # define the conditional probability dictionary for storing the result\n",
    "        self.conditional_probability = {}\n",
    "        # go through the entire vocabulary for initializing the probability as 0.0 to them (would compute afterward)\n",
    "        for token in self.vocabulary:\n",
    "            # go through the label as a set to assigning the label to them \n",
    "            for label in set(labels):\n",
    "                # create the labels' value for each token within the dictionary\n",
    "                self.conditional_probability[token] = {label : 0.0}\n",
    "        \n",
    "        # compute the conditional probability (prepare for naive bayes)\n",
    "        for label in set(labels):\n",
    "            # access the text class by according label\n",
    "            text_c = text_class[label]\n",
    "            # get the length of the those documents belong to the according label\n",
    "            total_token = len(text_c)\n",
    "            # count the frequency of each token belong to the according label\n",
    "            token_frequency = Counter(text_c)\n",
    "            # loop the entire vocabulary to compute their conditional probability by using smoothing approach\n",
    "            for token in self.vocabulary:\n",
    "                # access the frequency of the according token from the vocabulary\n",
    "                t_ct = token_frequency[token]\n",
    "                # compute the conditional probability based on the given formula\n",
    "                self.conditional_probability[token][label] = (t_ct + 1) / (total_token + len(self.vocabulary))\n",
    "\n",
    "    def tokenize(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to tokenize\n",
    "        :return: document as a list of tokens\n",
    "        '''\n",
    "        tokens = []\n",
    "        # tokenize the document by using the given regex and make every character as lowercase within the document\n",
    "        for token in re.findall(r\"\\b\\w\\w+\\b\", doc):\n",
    "            tokens.append(token.lower())\n",
    "        return tokens\n",
    "\n",
    "    def classify(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to classify\n",
    "        :return: most probable class\n",
    "        '''\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T22:07:25.031529Z",
     "start_time": "2023-10-28T22:07:25.002930Z"
    }
   },
   "id": "bc04efe66c9edcff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "__d) Evaluation (2pts)__\n",
    "Test your implementation on the 20newsgroups dataset. If implemented correctly, with `min_count=1` your Naive Bayes classifier should obtain the same accuracy as the implementation by scikit-learn (see below for comparison)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6046c75816d1c0c3"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "train_data = train.data\n",
    "train_label = train.target\n",
    "\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "test_data = test.data\n",
    "test_label = test.target\n",
    "\n",
    "# Training our custom NaiveBayesClassifier\n",
    "nb_classifier = NaiveBayesClassifier(train_data, train_label, min_count=1)\n",
    "predicted_labels_custom = [nb_classifier.classify(doc) for doc in test_data]\n",
    "accuracy_custom = accuracy_score(test_label, predicted_labels_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T22:08:40.751085Z",
     "start_time": "2023-10-28T22:07:25.911824Z"
    }
   },
   "id": "29e2da8da415d201"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# see https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html for details\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(train.data)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x,train.target)\n",
    "\n",
    "pred = clf.predict(vectorizer.transform(test.data))\n",
    "\n",
    "accuracy_score(test.target,pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e112ae88d9e43047"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### References"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8035984acf25ce1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "References for Task 3\n",
    "\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "* https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67\n",
    "* https://medium.com/@johnm.kovachi/implementing-a-multinomial-naive-bayes-classifier-from-scratch-with-python-e70de6a3b92e"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "478585d2613a7606"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
